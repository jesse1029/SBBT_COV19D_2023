{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, random,gc\n",
    "import copy, cv2\n",
    "pd.options.mode.chained_assignment = None\n",
    "import torch, torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_01 = pd.read_csv('./csv_pickle/df_test_224_cs_256_0.csv')\n",
    "# df_test_02 = pd.read_csv('./csv_pickle/df_test_224_cs_256_1.csv')\n",
    "# df_test_03 = pd.read_csv('./csv_pickle/df_test_224_cs_256_2.csv')\n",
    "# df_test_04 = pd.read_csv('./csv_pickle/df_test_224_cs_256_3.csv')\n",
    "# df = pd.concat([df_test_01,df_test_02,df_test_03,df_test_04])\n",
    "# df.to_csv('./csv_pickle/df_test_sz_256_224.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    model_path1 = \"/ssd8/2023COVID19/CT-COVID19-Classification/train_code_fix/best_model/f1_best_model_convembed.bin\"\n",
    "    # model_path1 = \"/ssd8/2023COVID19/CT-COVID19-Classification/inference_code/best_model/weight_from_144server/f1_best_model_k1_convembed_check_from144weight[384].bin\"\n",
    "    # model_path1 = \"/ssd8/2023COVID19/CT-COVID19-Classification/inference_code/best_model/weight_from_144server/f1_best_model_k1_convslice_check_from144weight[384].bin\"\n",
    "    # model_path1 = \"/ssd8/2023COVID19/CT-COVID19-Classification/train_code_fix/best_model/f1_best_model_k1_convslice.bin\"\n",
    "    pre_train = False\n",
    "    N_EPOCHS = 100\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 32\n",
    "    ct_len_get = 100 #100\n",
    "    kernal_size = 1 #1\n",
    "    SEDD =42\n",
    "    LR = 3e-5 #3e-5\n",
    "    WEIGHT_DECAY = 1e-3 #1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,os\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, ct_len=224, kernal_size = 3, pre_train=True):\n",
    "        super(MyModel, self).__init__()\n",
    "        # self.conv1d = nn.Conv1d(in_channels=100, out_channels=CONFIG.ct_len_get, kernel_size=kernal_size)\n",
    "        self.conv1d = nn.Conv1d(in_channels=224, out_channels=CONFIG.ct_len_get, kernel_size=kernal_size)\n",
    "        self.backbone = timm.create_model('resnet18', pretrained=pre_train, num_classes=1)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1d(x)\n",
    "        \n",
    "        x = torch.cat((x.unsqueeze(1), x.unsqueeze(1), x.unsqueeze(1)), dim=1)\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改init讀取全部embed的方法（記憶體無法負擔所有npy資料），改以呼叫index階段時再進行讀取\n",
    "class COVID_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, data_split, ct_len_s=50 , transform = None):\n",
    "        if data_split == 'test':\n",
    "            dataset = pd.read_csv(csv_path)\n",
    "            self.embed_info = dataset\n",
    "            classes = [999]\n",
    "            classes = sorted(list(classes))\n",
    "            class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "            ct_path = np.unique(dataset.iloc[:, 4])\n",
    "            imgs = []\n",
    "            for i_scan_dir in tqdm(ct_path):\n",
    "                temp_df = dataset[dataset['ct_path'] == i_scan_dir]\n",
    "                imgs.append((i_scan_dir, 999))\n",
    "            \n",
    "        elif data_split == 'train' or 'valid': \n",
    "            df = pd.read_csv(csv_path)\n",
    "            # df['embed'] = self.load_npy(df.ct_path.values.tolist(), df.embed.values.tolist())\n",
    "            dataset = df[df['split'] == data_split]\n",
    "            self.embed_info = dataset\n",
    "            classes = set(dataset['label'])\n",
    "            classes = sorted(list(classes))\n",
    "            class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "            \n",
    "            ct_path = np.unique(dataset.iloc[:, 4])\n",
    "            imgs = []\n",
    "            for i_scan_dir in ct_path:\n",
    "                temp_df = dataset[dataset['ct_path'] == i_scan_dir]\n",
    "                imgs.append((i_scan_dir, temp_df.iloc[0, 3]))\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.ct_len_s = ct_len_s\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "    def load_npy(self, npy_path, npy_file):\n",
    "        new_npy_embed = []\n",
    "        for path_, file_ in zip(npy_path, npy_file):\n",
    "            new_npy_embed.append(np.load(os.path.join(path_, file_)))\n",
    "        # print(np.array(new_npy_embed).shape)\n",
    "        return new_npy_embed\n",
    "    def __getitem__(self, index):\n",
    "        img_scan_dir, label = self.imgs[index]\n",
    "        # print(img_scan_dir, label)\n",
    "        label = self.class_to_idx[label]\n",
    "        temp_df = self.embed_info[self.embed_info['ct_path'] == img_scan_dir]\n",
    "        temp_df['embed'] = self.load_npy(temp_df['ct_path'].values.tolist(), temp_df['embed'].values.tolist())\n",
    "        random.seed(4019)\n",
    "        if len(temp_df) >= self.ct_len_s:\n",
    "            temp_index = [x for x in range(len(temp_df))]\n",
    "            target_index = random.sample(temp_index, k = self.ct_len_s)\n",
    "            \n",
    "        elif len(temp_df) < self.ct_len_s:\n",
    "            target_index = [x for x in range(len(temp_df))]\n",
    "            temp = random.choices(target_index, k = self.ct_len_s - len(target_index))\n",
    "            target_index += temp\n",
    "        \n",
    "        target_index.sort()\n",
    "        embed = temp_df.iloc[target_index, 1]\n",
    "        img = []\n",
    "        for i_embed in embed:\n",
    "            img.append(i_embed)\n",
    "        # img = np.expand_dims(np.array(img).reshape((1536, 8*8*self.ct_len_s)), axis=0)\n",
    "        img = np.array(img)\n",
    "        if len(img.shape)==4:\n",
    "            if img.shape[-1]==12:\n",
    "                img = np.array(img).reshape((1536, 12*12*self.ct_len_s))\n",
    "            elif img.shape[-1]==8:\n",
    "                img = np.array(img).reshape((1536, 8*8*self.ct_len_s))\n",
    "        else:\n",
    "            img = img.reshape((img.shape[1],img.shape[0]))\n",
    "        # img = np.concatenate([img,img,img], axis=0)\n",
    "        # print(img.shape)\n",
    "        return img, label, img_scan_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean 10\n",
    "def valid_one(model, loader):\n",
    "    losses, predicts = [], []\n",
    "    true_y=[]\n",
    "    pred_y=[]\n",
    "    paths = []\n",
    "    model.eval()\n",
    "    for images, label, path in loader:\n",
    "        with torch.no_grad():\n",
    "            images = images.cuda().float()\n",
    "            labels = label.cuda().float()\n",
    "            out = model(images)\n",
    "        predicts.append(out.cpu())\n",
    "        true_y.append(labels.cpu().numpy())\n",
    "        pred_y.append(torch.sigmoid(out).cpu().numpy())\n",
    "        paths.append(path)\n",
    "    true_y=np.concatenate(true_y)\n",
    "    pred_y=np.concatenate(pred_y)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    true_y=np.array(true_y).reshape(-1,1)\n",
    "    pred_y=np.array(pred_y).reshape(-1,1)\n",
    "\n",
    "    return true_y,pred_y,paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========data loader==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4308/4308 [05:20<00:00, 13.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_path = CONFIG.model_path1\n",
    "model = MyModel(ct_len = CONFIG.ct_len_get, kernal_size=CONFIG.kernal_size, pre_train=CONFIG.pre_train).cuda()\n",
    "model.load_state_dict(torch.load(pred_path))\n",
    "model.cuda()\n",
    "# df_path = './csv_pickle/df_224_embed_146server.csv'\n",
    "df_path = './csv_pickle/1d_2d_cnn_weight_from_sz384_144server/df_test_sz_256_224.csv'\n",
    "print(\"==========data loader==========\")\n",
    "valid_ds = COVID_Dataset(csv_path = df_path,data_split = 'test', ct_len_s = CONFIG.ct_len_get, transform = None)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=CONFIG.valid_batch_size, num_workers=15, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pred=[]\n",
    "for i in range(10):\n",
    "    true_y,pred_y, paths=valid_one(model, valid_loader)\n",
    "    total_pred.append(pred_y)\n",
    "# for i in range(len(total_pred)):\n",
    "#     print(f1_score(np.array(true_y),np.round(total_pred[i]),average='macro'))  \n",
    "# tn, fp, fn, tp = confusion_matrix(np.array(true_y), np.round(np.mean(total_pred,axis=0))).ravel()\n",
    "# print(\"Mean F1-Score: {}\".format(f1_score(np.array(true_y),np.round(np.mean(total_pred,axis=0)),average='macro')))\n",
    "# print(\"Negative Accuracy: {}\".format(tn/(tn+fp)))\n",
    "# print(\"Positive Accuracy: {}\".format(tp/(tp+fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.round(np.mean(total_pred,axis=0),5)>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "paths_list = list(itertools.chain(*paths))\n",
    "ct_name = []\n",
    "for i in paths_list:\n",
    "    ct_n = i.split('/')[-1]\n",
    "    ct_name.append(ct_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_conv_mix_S = pd.DataFrame([], columns=['CT_name', 'pred'])\n",
    "eff_conv_mix_S['pred'] = np.round(np.mean(total_pred,axis=0),5).flatten()\n",
    "eff_conv_mix_S['CT_name'] = ct_name\n",
    "eff_conv_mix_S.to_csv('./csv_pickle/eff_conv_mix_S_384_pres.csv', index=False)\n",
    "\n",
    "\n",
    "# eff_conv_mix_E = pd.DataFrame([], columns=['CT_name', 'pred'])\n",
    "# eff_conv_mix_E['pred'] = np.round(np.mean(total_pred,axis=0),5).flatten()\n",
    "# eff_conv_mix_E['CT_name'] = ct_name\n",
    "# eff_conv_mix_E.to_csv('./csv_pickle/eff_conv_mix_E_384_pres.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([275])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [384 x 384] embedding\n",
    "# 0.7 = 763\n",
    "# 0.6 = 796\n",
    "# 0.5 = 823\n",
    "# 0.4 = 849\n",
    "# 0.3 = 872\n",
    "# 0.2 = 913\n",
    "# 0.1 = 970\n",
    "# [384 x 384] sliece\n",
    "# 0.7 = 624\n",
    "# 0.6 = 655\n",
    "# 0.5 = 686\n",
    "# 0.4 = 714\n",
    "# 0.3 = 753\n",
    "# 0.2 = 800\n",
    "# 0.1 = 891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59460f4696914c2b63c32a491cda090056eb06068fec4b5dfed046fe694fcf36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
